{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas_profiling'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cff12cbd6e79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas_profiling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas_profiling'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "import calendar\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option('display.max_columns', None)  \n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import folium\n",
    "from folium import plugins\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry.polygon import Polygon\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Merge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = '/Users/anamaria/Desktop/dev/security_project/datasets/NUSE 934 611(M) 2017-2018.dsv'\n",
    "data2018=pd.read_csv(data_location,delimiter=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = '/Users/anamaria/Desktop/dev/security_project/datasets/NUSE 934-611-611M ENERO2019.csv'\n",
    "data2019=pd.read_csv(data_location,delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [data2018, data2019]\n",
    "data = pd.concat(frames)\n",
    "merged_nuse = data.loc[data['TIPO_DETALLE'] == '934 - RIÑA']\n",
    "merged_nuse.reset_index(inplace=True)\n",
    "merged_nuse.drop(columns=['index'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_nuse.to_csv(r'/Users/anamaria/Desktop/dev/security_project/datasets/merged_nuse.csv',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Rebuild missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "localidadCodDictionaryNuse = {1:'USAQUEN',\n",
    "                              2:'CHAPINERO',\n",
    "                              3:'SANTA FE',\n",
    "                              4:'SAN CRISTOBAL',\n",
    "                              5:'USME',\n",
    "                              6:'TUNJUELITO',\n",
    "                              7:'BOSA',\n",
    "                              8:'KENNEDY',\n",
    "                              9:'FONTIBON',\n",
    "                              10:'ENGATIVA',\n",
    "                              11:'SUBA',\n",
    "                              12:'BARRIOS UNIDOS',\n",
    "                              13:'TEUSAQUILLO',\n",
    "                              14:'LOS MARTIRES',\n",
    "                              15:'ANTONIO NARIÑO',\n",
    "                              16:'PUENTE ARANDA',\n",
    "                              17:'CANDELARIA',\n",
    "                              18:'RAFAEL URIBE URIBE',\n",
    "                              19:'CIUDAD BOLIVAR',\n",
    "                              20:'SUMAPAZ',\n",
    "                              99:'SIN LOCALIZACION'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods to rebuild"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from ws_address.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import ws_address\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import re\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_between( s, first, last ):\n",
    "    try:\n",
    "        start = s.index( first ) + len( first )\n",
    "        end = s.index( last, start )\n",
    "        return s[start:end]\n",
    "    except ValueError:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = [\"Dirección ingresada: \",\"Dirección encontrada: \",\"Tipo dirección: \",\"Código postal: \",\"Sector catastral: \",\n",
    "        \"UPZ: \",\"Localidad: \",\"Latitud: \",\"Longitud: \",\"CHIP: \"]\n",
    "def parse_address_ws(ws_result):\n",
    "    location = {}\n",
    "    for idx in range(len(tags)-1):\n",
    "        location[tags[idx].replace(': ','')] = find_between(ws_result,tags[idx],tags[idx+1])\n",
    "    return location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_upz(original_df,index,UPZ_ws_field):\n",
    "    original_df.at[index,'COD_UPZ'] = find_between(UPZ_ws_field, '(', ')')\n",
    "    original_df.at[index,'UPZ'] = find_between(UPZ_ws_field, '', ' (')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cod_localidad(localidad_name):\n",
    "    return [key  for (key, value) in localidadCodDictionaryNuse.items() if value == localidad_name][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebuild_location_in_nuse(original_df, index, driver):\n",
    "    address = original_df.at[index,'STR_DIRECCION_INCIDENTE']\n",
    "    print(address)\n",
    "    result_ws = ws_address.web_scrap_address(driver,address)\n",
    "    ws_address.delete_address(driver,address)\n",
    "\n",
    "    if result_ws != \"Not found\":\n",
    "        parsed_result = parse_address_ws(result_ws)\n",
    "        print(parsed_result)\n",
    "        if parsed_result[\"Dirección ingresada\"] != address:\n",
    "            return \"Error loading address\"\n",
    "        else:            \n",
    "            original_df.at[index,'LATITUD'] = float(parsed_result['Latitud'])\n",
    "            original_df.at[index,'LONGITUD'] = float(parsed_result['Longitud'])\n",
    "            parsed_localidad = parsed_result['Localidad']\n",
    "            if parsed_localidad == 'ANTONIO NARIÑO':\n",
    "                original_df.at[index,'LOCALIDAD'] = parsed_localidad\n",
    "            else:\n",
    "                original_df.at[index,'LOCALIDAD'] = unidecode.unidecode(parsed_localidad)\n",
    "            original_df.at[index,'COD_LOCALIDAD'] = int(get_cod_localidad(original_df.at[index,'LOCALIDAD']))\n",
    "            original_df.at[index,'SEC_CATASTRAL'] = parsed_result['Sector catastral']\n",
    "            assign_upz(original_df,index,parsed_result['UPZ'])\n",
    "            return \"Rebuilt\"\n",
    "    else:\n",
    "        return \"Not processed\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebuild_address_in_nuse(original_df, index):\n",
    "    log_text = original_df.at[index,'LOG_TEXT']\n",
    "    address_found = re.search(address_regex,log_text)\n",
    "\n",
    "    if address_found != None:\n",
    "        parsed_address = clean_address(address_found)\n",
    "        print(parsed_address.strip())\n",
    "        original_df.at[index,'STR_DIRECCION_INCIDENTE'] = parsed_address.strip()\n",
    "        return \"Rebuilt\"\n",
    "    else:\n",
    "        original_df.at[index,'STR_DIRECCION_INCIDENTE'] = 'ND'\n",
    "        return \"Not processed\"\n",
    "\n",
    "def clean_address(address_found):\n",
    "    exclude_char_list = ['~','/','*','(',')']\n",
    "    one_occurrence = address_found.group().split(',,,')[0].replace(',',' ')\n",
    "    final_address = one_occurrence\n",
    "    \n",
    "    for char in exclude_char_list:\n",
    "        if char in one_occurrence:\n",
    "            final_address = final_address.split(char)[0]\n",
    "            \n",
    "    numbers_in_substring = re.sub('[^0-9]','', final_address)\n",
    "    numbers_proportion = len(numbers_in_substring)/len(final_address)\n",
    "    \n",
    "    if numbers_proportion < 0.2:\n",
    "        final_address = 'ND'\n",
    "    \n",
    "    return final_address\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement rebuild methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = '/Users/anamaria/Desktop/dev/security_project/datasets/merged_nuse.csv'\n",
    "merged_nuse=pd.read_csv(data_location,delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"Tipo de dato\":merged_nuse.dtypes.values,\n",
    "              \"Celdas con valor '-'\":(merged_nuse == '-').sum().values,\n",
    "              \"Celdas con valor ''\":(merged_nuse == '').sum().values,\n",
    "              \"Celdas con valor ' '\":(merged_nuse == ' ').sum().values,\n",
    "              \"Celdas vacías\": merged_nuse.isna().sum().values},\n",
    "             index=merged_nuse.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rebuild address through log_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try to rebuild missing address through log_text field\n",
    "df_empty_locations_without_address = merged_nuse.loc[merged_nuse['STR_DIRECCION_INCIDENTE'] == '-']\n",
    "list_idx_rebuild_address = list(df_empty_locations_without_address.index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_idx_rebuild_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address_regex= '(CL|DG|KR|TV)+\\s\\d+.*(,,)'\n",
    "registers_to_process = len(list_idx_rebuild_address)\n",
    "rebuilt_registers = 0\n",
    "registers_not_processed = 0\n",
    "other_condition_counter = 0\n",
    "\n",
    "for index in list_idx_rebuild_address:\n",
    "    state = rebuild_address_in_nuse(merged_nuse, index)\n",
    "    \n",
    "    if state == \"Rebuilt\":\n",
    "        rebuilt_registers += 1\n",
    "    elif state == \"Not processed\":\n",
    "        registers_not_processed += 1\n",
    "    else:\n",
    "        other_condition_counter += 1\n",
    "    \n",
    "    print('Rebuilt registers: ',rebuilt_registers,'/',registers_to_process)\n",
    "    print('Registers not processed: ',registers_not_processed, '/', registers_to_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_nuse.to_csv(r'/Users/anamaria/Desktop/dev/security_project/datasets/rebuild_address_nuse_18112019.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"Tipo de dato\":merged_nuse.dtypes.values,\n",
    "              \"Celdas con valor '-'\":(merged_nuse == '-').sum().values,\n",
    "              \"Celdas con valor 'ND'\":(merged_nuse == 'ND').sum().values,\n",
    "              \"Celdas vacías\": merged_nuse.isna().sum().values},\n",
    "             index=merged_nuse.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rebuild location through address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_location = '/Users/anamaria/Desktop/dev/security_project/datasets/rebuild_address_nuse_18112019.csv'\n",
    "data_location = '/Users/anamaria/Desktop/dev/security_project/datasets/standardise_result_nuse_27112019.csv'\n",
    "df_input = pd.read_csv(data_location,delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_input.loc[df_input['COD_UPZ'] == '-']\n",
    "df2 = df_input.loc[df_input['UPZ'] == '-']\n",
    "df3 = df_input.loc[df_input['COD_SEC_CATAST'] == '-']\n",
    "df4 = df_input.loc[df_input['SEC_CATASTRAL'] == '-']\n",
    "df5 = df_input.loc[df_input['COD_BARRIO'] == '-']\n",
    "df6 = df_input.loc[df_input['BARRIO'] == '-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.equals(df2) and df1.equals(df3) and df1.equals(df4) and df1.equals(df5) and df1.equals(df6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "755"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Try to rebuild 'sector catastral', 'UPZ', 'localidad', 'latitud', 'longitud' through address\n",
    "df_empty_locations_with_address = df1.loc[df1['STR_DIRECCION_INCIDENTE'] != 'ND']\n",
    "list_idx_rebuild_location = list(df_empty_locations_with_address.index.values)\n",
    "len(list_idx_rebuild_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rebuild 'sector catastral', 'UPZ', 'localidad', 'latitud', 'longitud' using web scraping\n",
    "df_output = df_input\n",
    "url='https://mapas.bogota.gov.co'\n",
    "driver = ws_address.web_scrap_page(url)\n",
    "registers_to_process = len(list_idx_rebuild_location)\n",
    "rebuilt_registers = 0\n",
    "registers_not_processed = 0\n",
    "other_condition_counter = 0\n",
    "idx_error_loading_address = []\n",
    "\n",
    "for index in list_idx_rebuild_location:\n",
    "    state = rebuild_location_in_nuse(df_output, index, driver)\n",
    "    \n",
    "    if state == \"Rebuilt\":\n",
    "        rebuilt_registers += 1\n",
    "    elif state == \"Not processed\":\n",
    "        registers_not_processed += 1\n",
    "    elif state == \"Error loading address\":\n",
    "        idx_error_loading_address.append(index)\n",
    "    else:\n",
    "        other_condition_counter += 1\n",
    "    \n",
    "    print('Rebuilt registers: ',rebuilt_registers,'/',registers_to_process)\n",
    "    print('Registers not processed: ',registers_not_processed, '/', registers_to_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[163]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_error_loading_address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate over idx_error_loading_address to rebuild location if necesary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rebuilt_registers)\n",
    "print(registers_not_processed)\n",
    "print(other_condition_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output.to_csv(r'/Users/anamaria/Desktop/dev/security_project/datasets/rebuild_locations_nuse_29112019.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"Tipo de dato\":df_output.dtypes.values,\n",
    "              \"Celdas con valor '-'\":(df_output == '-').sum().values,\n",
    "              \"Celdas con valor 'ND'\":(df_output == 'ND').sum().values,\n",
    "              \"Celdas vacías\": df_output.isna().sum().values},\n",
    "             index=df_output.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign ND to df_empty_locations_without_address on location fields\n",
    "#'SEC_CATASTRAL', 'UPZ', 'COD_UPZ', 'LATITUD'', 'LONGITUD', 'LOCALIDAD', 'COD_LOCALIDAD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/combios/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "data_location = '/home/combios/Documents/amreyesp/clean_nuse_data/rebuild_locations_nuse_29012020.csv'\n",
    "df_input = pd.read_csv(data_location,delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3139"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Registers without address or coordinates can not be rebuilt\n",
    "df_empty_locations_without_address = df_input.loc[(df_input['STR_DIRECCION_INCIDENTE'] == 'ND') & (df_input['LATITUD']==-1) & (df_input['LONGITUD']==-1)]\n",
    "list_idx_not_rebuild = list(df_empty_locations_without_address.index.values)\n",
    "len(list_idx_not_rebuild)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output = df_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in list_idx_not_rebuild:\n",
    "    #df_output.at[index,'LATITUD'] = 'ND'\n",
    "    #df_output.at[index,'LONGITUD'] = 'ND'\n",
    "    df_output.at[index,'SEC_CATASTRAL'] = 'ND'\n",
    "    df_output.at[index,'UPZ'] = 'ND'\n",
    "    df_output.at[index,'COD_UPZ'] = 'ND'\n",
    "    df_output.at[index,'LOCALIDAD'] = 'SIN LOCALIZACION'\n",
    "    df_output.at[index,'LATITUD'] = 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output.to_csv(r'/home/combios/Documents/amreyesp/clean_nuse_data/rebuild_nuse_29012020.csv',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Standardise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/combios/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "data_location = '/home/combios/Documents/amreyesp/rebuild_locations_nuse_idx_30kto39k_23012020.csv'\n",
    "df_input = pd.read_csv(data_location,delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create timpestamp col to handle time ranges on unique event process\n",
    "df_input['time_stamp']=pd.to_datetime(df_input['FECHA'] + ' ' + df_input[\"HORA\"].astype(str).str.rjust(4,'0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/combios/anaconda3/lib/python3.7/site-packages/pandas/core/ops/__init__.py:1115: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  result = method(y)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tipo de dato</th>\n",
       "      <th>Celdas con valor '-'</th>\n",
       "      <th>Celdas con valor 'ND'</th>\n",
       "      <th>Celdas vacías</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>STR_NUMERO_INTERNO</td>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>FECHA</td>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>HORA</td>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ANIO</td>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>MES</td>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>COD_LOCALIDAD</td>\n",
       "      <td>object</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>LOCALIDAD</td>\n",
       "      <td>object</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>COD_UPZ</td>\n",
       "      <td>object</td>\n",
       "      <td>28041</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>UPZ</td>\n",
       "      <td>object</td>\n",
       "      <td>28041</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>COD_SEC_CATAST</td>\n",
       "      <td>object</td>\n",
       "      <td>51950</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>SEC_CATASTRAL</td>\n",
       "      <td>object</td>\n",
       "      <td>28041</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>COD_BARRIO</td>\n",
       "      <td>object</td>\n",
       "      <td>51950</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>BARRIO</td>\n",
       "      <td>object</td>\n",
       "      <td>51950</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>TIPO_UNICO</td>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>TIPO_DETALLE</td>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>LATITUD</td>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>LONGITUD</td>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>STR_DIRECCION_INCIDENTE</td>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>105469</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ESTADO_INCIDENTE</td>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>PERIODO_TS</td>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>LOG_TEXT</td>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Tipo de dato  Celdas con valor '-'  \\\n",
       "STR_NUMERO_INTERNO            object                     0   \n",
       "FECHA                         object                     0   \n",
       "HORA                           int64                     0   \n",
       "ANIO                           int64                     0   \n",
       "MES                            int64                     0   \n",
       "COD_LOCALIDAD                 object                     3   \n",
       "LOCALIDAD                     object                     3   \n",
       "COD_UPZ                       object                 28041   \n",
       "UPZ                           object                 28041   \n",
       "COD_SEC_CATAST                object                 51950   \n",
       "SEC_CATASTRAL                 object                 28041   \n",
       "COD_BARRIO                    object                 51950   \n",
       "BARRIO                        object                 51950   \n",
       "TIPO_UNICO                     int64                     0   \n",
       "TIPO_DETALLE                  object                     0   \n",
       "LATITUD                      float64                     0   \n",
       "LONGITUD                     float64                     0   \n",
       "STR_DIRECCION_INCIDENTE       object                     0   \n",
       "ESTADO_INCIDENTE              object                     0   \n",
       "PERIODO_TS                    object                     0   \n",
       "LOG_TEXT                      object                     0   \n",
       "\n",
       "                         Celdas con valor 'ND'  Celdas vacías  \n",
       "STR_NUMERO_INTERNO                           0              0  \n",
       "FECHA                                        0              0  \n",
       "HORA                                         0              0  \n",
       "ANIO                                         0              0  \n",
       "MES                                          0              0  \n",
       "COD_LOCALIDAD                                0              0  \n",
       "LOCALIDAD                                    0              0  \n",
       "COD_UPZ                                      0             45  \n",
       "UPZ                                          0             45  \n",
       "COD_SEC_CATAST                               0              0  \n",
       "SEC_CATASTRAL                                0             17  \n",
       "COD_BARRIO                                   0              0  \n",
       "BARRIO                                       0              0  \n",
       "TIPO_UNICO                                   0              0  \n",
       "TIPO_DETALLE                                 0              0  \n",
       "LATITUD                                      0              0  \n",
       "LONGITUD                                     0              0  \n",
       "STR_DIRECCION_INCIDENTE                 105469              0  \n",
       "ESTADO_INCIDENTE                             0            103  \n",
       "PERIODO_TS                                   0              0  \n",
       "LOG_TEXT                                     0              0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\"Tipo de dato\":df_input.dtypes.values,\n",
    "              \"Celdas con valor '-'\":(df_input == '-').sum().values,\n",
    "              \"Celdas con valor 'ND'\":(df_input == 'ND').sum().values,\n",
    "              \"Celdas vacías\": df_input.isna().sum().values},\n",
    "             index=df_input.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 One register per event: event that occurs within 400 mts radius and 20 minutes time interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find duplicated events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, datetime\n",
    "time_offset = 20\n",
    "coor_offset = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_duplicated_events(df, row):\n",
    "    current_time = row['time_stamp']\n",
    "    current_lat = row['LATITUD']\n",
    "    current_lon = row['LONGITUD']\n",
    "    current_point=Point(current_lon,current_lat)\n",
    "\n",
    "    duplicated_event_idx = {}\n",
    "    limit_time_interval = current_time + datetime.timedelta(minutes = time_offset)\n",
    "    df_event_time = df.loc[(df['time_stamp'] >= current_time) & (df['time_stamp'] < limit_time_interval)]\n",
    "    \n",
    "    lat_point_list = [current_lat-coor_offset, current_lat-coor_offset, current_lat+coor_offset, current_lat+coor_offset]\n",
    "    lon_point_list = [current_lon+coor_offset, current_lon-coor_offset, current_lon-coor_offset, current_lon+coor_offset]\n",
    "    polygon_event = Polygon(zip(lon_point_list, lat_point_list))\n",
    "    \n",
    "    for index, row in df_event_time.iterrows():\n",
    "        point=Point(row['LONGITUD'],row['LATITUD'])\n",
    "        if point.within(polygon_event):\n",
    "            #duplicated_event_idx.append(index)\n",
    "            duplicated_event_idx[index] = row['STR_NUMERO_INTERNO']\n",
    "    return duplicated_event_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output = df_input.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output['dup_event'] = df_output.apply (lambda row: find_duplicated_events(df_output, row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output.to_csv(r'/Users/anamaria/Desktop/dev/security_project/datasets/standardise_find_dup_event_nuse_26112019.csv',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete duplicated events: preserve the first event on dup_event column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = '/Users/anamaria/Desktop/dev/security_project/datasets/standardise_find_dup_event_nuse_26112019.csv'\n",
    "df_input = pd.read_csv(data_location,delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"Tipo de dato\":df_input.dtypes.values,\n",
    "              \"Celdas con valor '-'\":(df_input == '-').sum().values,\n",
    "              \"Celdas con valor 'ND'\":(df_input == 'ND').sum().values,\n",
    "              \"Celdas vacías\": df_input.isna().sum().values},\n",
    "             index=df_input.columns)\n",
    "print(df_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Get index of registers that should be deleted\n",
    "import ast\n",
    "df = df_input\n",
    "list_idx_repeated = []\n",
    "list_idx_preserved = []\n",
    "registers_to_process = len(df)\n",
    "list_idx_processed =[]\n",
    "counter_processed = 0\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    dup_event_x = ast.literal_eval(df.at[index,'dup_event'])\n",
    "    current_dup_events = list(dup_event_x.keys())\n",
    "\n",
    "    if (current_dup_events[0] not in list_idx_processed) & (current_dup_events[0] not in list_idx_preserved):\n",
    "        list_idx_preserved.append(current_dup_events[0])\n",
    "        list_idx_processed.append(current_dup_events[0])\n",
    "        current_dup_events.pop(0)\n",
    "\n",
    "    for idx_event in current_dup_events:\n",
    "        if idx_event not in list_idx_processed:\n",
    "            list_idx_repeated.append(idx_event)\n",
    "            list_idx_processed.append(idx_event)\n",
    "                \n",
    "    counter_processed += 1\n",
    "    \n",
    "    print('Registers processed: ',counter_processed,'/',registers_to_process)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check (quantitatively) ID of preserved and repeated events index was succesful\n",
    "print(len(list_idx_repeated)+len(list_idx_preserved))\n",
    "print(len(list_idx_processed))\n",
    "join_list = list_idx_preserved + list_idx_repeated\n",
    "\n",
    "import collections\n",
    "seen = set()\n",
    "uniq = []\n",
    "for x in join_list:\n",
    "    if x not in seen:\n",
    "        uniq.append(x)\n",
    "        seen.add(x)\n",
    "\n",
    "print(len(uniq))\n",
    "\n",
    "lst = join_list\n",
    "dupItems = []\n",
    "uniqItems = {}\n",
    "for x in lst:\n",
    "    if x not in uniqItems:\n",
    "        uniqItems[x] = 1\n",
    "    else:\n",
    "        if uniqItems[x] == 1:\n",
    "            dupItems.append(x)\n",
    "        uniqItems[x] += 1\n",
    "        \n",
    "print(len(dupItems))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output = df_input.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output=df_output.drop(list_idx_repeated)\n",
    "df_output.drop(columns=['dup_event','time_stamp'],inplace=True)\n",
    "df_output.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output.to_csv(r'/Users/anamaria/Desktop/dev/security_project/datasets/standardise_result_nuse_27112019.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save lists\n",
    "MyFile=open('/Users/anamaria/Desktop/dev/security_project/datasets/list_idx_preserved_27112019.txt','w')\n",
    "MyList=map(lambda x: str(x)+'\\n', list_idx_preserved)\n",
    "MyFile.writelines(MyList)\n",
    "MyFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save lists\n",
    "MyFile=open('/Users/anamaria/Desktop/dev/security_project/datasets/list_idx_repeated_27112019.txt','w')\n",
    "MyList=map(lambda x: str(x)+'\\n', list_idx_repeated)\n",
    "MyFile.writelines(MyList)\n",
    "MyFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Normalise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_location = '/Users/anamaria/Desktop/dev/security_project/datasets/standardise_result_nuse_27112019.csv'\n",
    "data_location = '/Users/anamaria/Desktop/dev/security_project/datasets/rebuild_locations_nuse_29112019.csv'\n",
    "df_input = pd.read_csv(data_location,delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_input.shape)\n",
    "pd.DataFrame({\"Tipo de dato\":df_input.dtypes.values,\n",
    "              \"Celdas con valor '-'\":(df_input == '-').sum().values,\n",
    "              \"Celdas con valor 'ND'\":(df_input == 'ND').sum().values,\n",
    "              \"Celdas vacías\": df_input.isna().sum().values},\n",
    "             index=df_input.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify FECHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It´s a REGEX with the form: nn-www-nnnn\n",
    "regex_fecha = '^\\d{1,2}-\\w{3}-\\d{1,2}$'\n",
    "df_input['FECHA'].str.contains(regex_fecha, regex=True).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify HORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It´s a number between 0 and 2359\n",
    "print(df_input['HORA'].between(0,2359).all())\n",
    "\n",
    "# It´s a regex:\n",
    "regex_hora = '^[0-2][0-9][0-5]|[0-9]$'\n",
    "df_input['HORA'].apply(str).str.contains(regex_hora, regex=True).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify ANIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It´s a number between 2017 and 2019\n",
    "df_input['ANIO'].between(2017,2019).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify MES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It´s a number between 1 and 12\n",
    "df_input['MES'].between(1,12).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify COD_LOCALIDAD - LOCALIDAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_aux = 'STR_NUMERO_INTERNO'\n",
    "df_input.groupby(['COD_LOCALIDAD','LOCALIDAD']).agg({var_aux:'count'}).reset_index().rename(columns={var_aux:'Frecuencia'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify LATITUD, LONGITUD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should be in Bogotá\n",
    "json_file=\"/Users/anamaria/Desktop/dev/security_project/assets/bogota_polygon.geojson\"\n",
    "bog_loc=gpd.read_file(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output=df_input.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_bog_location(df, row):\n",
    "    lat = row['LATITUD']\n",
    "    lon = row['LONGITUD']\n",
    "    current_point = Point(lon,lat)\n",
    "    if bog_loc.geometry.contains(current_point)[0]:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output['in_bogota?'] = df_output.apply (lambda row: check_bog_location(df_output, row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_output))\n",
    "print(len(df_output.loc[df_output['in_bogota?'] == True]))\n",
    "print(len(df_output.loc[df_output['in_bogota?'] == False]))\n",
    "print(len(df_output.loc[(df_output['in_bogota?'] == False) & (df_output['LATITUD']==-1) & (df_output['LONGITUD']==-1)]))\n",
    "print(len(df_output.loc[(df_output['in_bogota?'] == False) & (df_output['LATITUD']!=-1) & (df_output['LONGITUD']!=-1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output.loc[(df_output['in_bogota?'] == False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output.to_csv(r'/Users/anamaria/Desktop/dev/security_project/datasets/normalise_find_out_bogota_nuse_29112019.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get index of registers out of Bogota and drop it\n",
    "list_index_out_bogota=df_output[(df_output['in_bogota?'] == False)].index\n",
    "df_output=df_output.drop(list_index_out_bogota)\n",
    "df_output['in_bogota?'].all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output.to_csv(r'/Users/anamaria/Desktop/dev/security_project/datasets/normalise_result_nuse_29112019.csv',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. De-duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = '/Users/anamaria/Desktop/dev/security_project/datasets/normalise_result_nuse_29112019.csv'\n",
    "df_input = pd.read_csv(data_location,delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_input.shape)\n",
    "pd.DataFrame({\"Tipo de dato\":df_input.dtypes.values,\n",
    "              \"Celdas con valor '-'\":(df_input == '-').sum().values,\n",
    "              \"Celdas con valor 'ND'\":(df_input == 'ND').sum().values,\n",
    "              \"Celdas vacías\": df_input.isna().sum().values},\n",
    "             index=df_input.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify there are not identycal rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Filas duplicadas\",df_input.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify unique STR_NUMERO_INTERNO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_input) == len(df_input['STR_NUMERO_INTERNO'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input.to_csv(r'/Users/anamaria/Desktop/dev/security_project/datasets/deduplicate_nuse_29112019.csv',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Verify and enrich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = '/Users/anamaria/Desktop/dev/security_project/datasets/deduplicate_nuse_29112019.csv'\n",
    "df_input = pd.read_csv(data_location,delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_input.shape)\n",
    "pd.DataFrame({\"Tipo de dato\":df_input.dtypes.values,\n",
    "              \"Celdas con valor '-'\":(df_input == '-').sum().values,\n",
    "              \"Celdas con valor 'ND'\":(df_input == 'ND').sum().values,\n",
    "              \"Celdas vacías\": df_input.isna().sum().values},\n",
    "             index=df_input.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output=df_input.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify columns with empty or anomalous values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check COD_UPZ, UPZ, SEC_CATASTRAL with '-' values\n",
    "df1 = df_output.loc[df_output['COD_UPZ']=='-']\n",
    "df2 = df_output.loc[df_output['UPZ']=='-']\n",
    "df3 = df_output.loc[df_output['SEC_CATASTRAL']=='-']\n",
    "df1.equals(df2) and df1.equals(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check COD_UPZ, UPZ, SEC_CATASTRAL with empty values\n",
    "df_output.loc[df_output['COD_UPZ'].isna(),'COD_UPZ'] = '-'\n",
    "df_output.loc[df_output['UPZ'].isna(),'UPZ'] = '-'\n",
    "df_output.loc[df_output['SEC_CATASTRAL'].isna(),'SEC_CATASTRAL'] = '-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check ESTADO_INCIDENTE with empty values\n",
    "df_output['ESTADO_INCIDENTE'].value_counts()\n",
    "#rebuild empty values with 'CERRADO'\n",
    "df_output.loc[df_output['ESTADO_INCIDENTE'].isna(),'ESTADO_INCIDENTE'] = 'CERRADO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check BARRIO and COD_BARRIO with '-' values\n",
    "df1 = df_output.loc[df_output['BARRIO']=='-']\n",
    "df2 = df_output.loc[df_output['COD_BARRIO']=='-']\n",
    "df3 = df_output.loc[df_output['COD_SEC_CATAST']=='-']\n",
    "df1.equals(df2) and df1.equals(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check STR_DIRECCION_INCIDENTE with 'ND' values\n",
    "df_output.loc[(df_output['STR_DIRECCION_INCIDENTE'] == 'ND')]\n",
    "df_output.loc[(df_output['STR_DIRECCION_INCIDENTE'] == 'ND') & (df_output['COD_LOCALIDAD'] == 99)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete aditional columns created on cleaning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output.drop(columns=['index','in_bogota?'],inplace=True)\n",
    "df_output.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output.drop(columns=['index'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_output.shape)\n",
    "pd.DataFrame({\"Tipo de dato\":df_output.dtypes.values,\n",
    "              \"Celdas con valor '-'\":(df_output == '-').sum().values,\n",
    "              \"Celdas con valor 'ND'\":(df_output == 'ND').sum().values,\n",
    "              \"Celdas vacías\": df_output.isna().sum().values},\n",
    "             index=df_output.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output.to_csv(r'/Users/anamaria/Desktop/dev/security_project/datasets/verify_enrich_nuse_29112019.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
