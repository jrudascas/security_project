{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "import calendar\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option('display.max_columns', None)  \n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import folium\n",
    "from folium import plugins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Merge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = '/Users/anamaria/Desktop/dev/security_project/datasets/NUSE 934 611(M) 2017-2018.dsv'\n",
    "data2018=pd.read_csv(data_location,delimiter=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = '/Users/anamaria/Desktop/dev/security_project/datasets/NUSE 934-611-611M ENERO2019.csv'\n",
    "data2019=pd.read_csv(data_location,delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [data2018, data2019]\n",
    "data = pd.concat(frames)\n",
    "merged_nuse = data.loc[data['TIPO_DETALLE'] == '934 - RIÑA']\n",
    "merged_nuse.reset_index(inplace=True)\n",
    "merged_nuse.drop(columns=['index'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "merged_nuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_nuse.to_csv(r'/Users/anamaria/Desktop/dev/security_project/datasets/merged_nuse.csv',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Rebuild missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods to rebuild"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import ws_address\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_between( s, first, last ):\n",
    "    try:\n",
    "        start = s.index( first ) + len( first )\n",
    "        end = s.index( last, start )\n",
    "        return s[start:end]\n",
    "    except ValueError:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = [\"Dirección ingresada: \",\"Dirección encontrada: \",\"Tipo dirección: \",\"Código postal: \",\"Sector catastral: \",\n",
    "        \"UPZ: \",\"Localidad: \",\"Latitud: \",\"Longitud: \",\"CHIP: \"]\n",
    "def parse_address_ws(ws_result):\n",
    "    location = {}\n",
    "    for idx in range(len(tags)-1):\n",
    "        location[tags[idx].replace(': ','')] = find_between(ws_result,tags[idx],tags[idx+1])\n",
    "    return location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_upz(original_df,index,UPZ_ws_field):\n",
    "    original_df.at[index,'COD_UPZ'] = find_between(UPZ_ws_field, '(', ')')\n",
    "    original_df.at[index,'UPZ'] = find_between(UPZ_ws_field, '', ' (')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebuild_location_in_nuse(original_df, index, driver):\n",
    "    address = original_df.at[index,'STR_DIRECCION_INCIDENTE']\n",
    "    print(address)\n",
    "    result_ws = ws_address.web_scrap_address(driver,address)\n",
    "    ws_address.delete_address(driver,address)\n",
    "    print(result_ws)\n",
    "\n",
    "    if result_ws != \"Not found\":\n",
    "        parsed_result = parse_address_ws(result_ws)\n",
    "        print(parsed_result)\n",
    "        original_df.at[index,'LATITUD'] = float(parsed_result['Latitud'])\n",
    "        original_df.at[index,'LONGITUD'] = float(parsed_result['Longitud'])\n",
    "        original_df.at[index,'LOCALIDAD'] = parsed_result['Localidad']\n",
    "        #original_df.at[index,'COD_LOCALIDAD'] = '-'\n",
    "        original_df.at[index,'SEC_CATASTRAL'] = parsed_result['Sector catastral']\n",
    "        assign_upz(original_df,index,parsed_result['UPZ'])\n",
    "        return \"Rebuilt\"\n",
    "    else:\n",
    "        return \"Not processed\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebuild_address_in_nuse(original_df, index):\n",
    "    log_text = original_df.at[index,'LOG_TEXT']\n",
    "    address_found = re.search(address_regex,log_text)\n",
    "\n",
    "    if address_found != None:\n",
    "        parsed_address = clean_address(address_found)\n",
    "        print(parsed_address.strip())\n",
    "        original_df.at[index,'STR_DIRECCION_INCIDENTE'] = parsed_address.strip()\n",
    "        return \"Rebuilt\"\n",
    "    else:\n",
    "        original_df.at[index,'STR_DIRECCION_INCIDENTE'] = 'ND'\n",
    "        return \"Not processed\"\n",
    "\n",
    "def clean_address(address_found):\n",
    "    exclude_char_list = ['~','/','*','(',')']\n",
    "    one_occurrence = address_found.group().split(',,,')[0].replace(',',' ')\n",
    "    final_address = one_occurrence\n",
    "    \n",
    "    for char in exclude_char_list:\n",
    "        if char in one_occurrence:\n",
    "            final_address = final_address.split(char)[0]\n",
    "            \n",
    "    numbers_in_substring = re.sub('[^0-9]','', final_address)\n",
    "    numbers_proportion = len(numbers_in_substring)/len(final_address)\n",
    "    \n",
    "    if numbers_proportion < 0.2:\n",
    "        final_address = 'ND'\n",
    "    \n",
    "    return final_address\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement rebuild methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = '/Users/anamaria/Desktop/dev/security_project/datasets/merged_nuse.csv'\n",
    "merged_nuse=pd.read_csv(data_location,delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"Tipo de dato\":merged_nuse.dtypes.values,\n",
    "              \"Celdas con valor '-'\":(merged_nuse == '-').sum().values,\n",
    "              \"Celdas con valor ''\":(merged_nuse == '').sum().values,\n",
    "              \"Celdas con valor ' '\":(merged_nuse == ' ').sum().values,\n",
    "              \"Celdas vacías\": merged_nuse.isna().sum().values},\n",
    "             index=merged_nuse.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = merged_nuse.loc[merged_nuse['COD_UPZ'] == '-']\n",
    "df2 = merged_nuse.loc[merged_nuse['UPZ'] == '-']\n",
    "df3 = merged_nuse.loc[merged_nuse['COD_SEC_CATAST'] == '-']\n",
    "df4 = merged_nuse.loc[merged_nuse['SEC_CATASTRAL'] == '-']\n",
    "df5 = merged_nuse.loc[merged_nuse['COD_BARRIO'] == '-']\n",
    "df6 = merged_nuse.loc[merged_nuse['BARRIO'] == '-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.equals(df2) and df1.equals(df3) and df1.equals(df4) and df1.equals(df5) and df1.equals(df6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rebuild address through log_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try to rebuild missing address through log_text field\n",
    "df_empty_locations_without_address = merged_nuse.loc[merged_nuse['STR_DIRECCION_INCIDENTE'] == '-']\n",
    "list_idx_rebuild_address = list(df_empty_locations_without_address.index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_idx_rebuild_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address_regex= '(CL|DG|KR|TV)+\\s\\d+.*(,,)'\n",
    "registers_to_process = len(list_idx_rebuild_address)\n",
    "rebuilt_registers = 0\n",
    "registers_not_processed = 0\n",
    "other_condition_counter = 0\n",
    "\n",
    "for index in list_idx_rebuild_address:\n",
    "    state = rebuild_address_in_nuse(rebuild_nuse1, index)\n",
    "    \n",
    "    if state == \"Rebuilt\":\n",
    "        rebuilt_registers += 1\n",
    "    elif state == \"Not processed\":\n",
    "        registers_not_processed += 1\n",
    "    else:\n",
    "        other_condition_counter += 1\n",
    "    \n",
    "    print('Rebuilt registers: ',rebuilt_registers,'/',registers_to_process)\n",
    "    print('Registers not processed: ',registers_not_processed, '/', registers_to_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_nuse.to_csv(r'/Users/anamaria/Desktop/dev/security_project/datasets/rebuild_address_nuse_18112019.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"Tipo de dato\":merged_nuse.dtypes.values,\n",
    "              \"Celdas con valor '-'\":(merged_nuse == '-').sum().values,\n",
    "              \"Celdas con valor 'ND'\":(merged_nuse == 'ND').sum().values,\n",
    "              \"Celdas vacías\": merged_nuse.isna().sum().values},\n",
    "             index=merged_nuse.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rebuild location through address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try to rebuild 'sector catastral', 'UPZ', 'localidad', 'latitud', 'longitud' through address\n",
    "df_empty_locations_with_address = df1.loc[df1['STR_DIRECCION_INCIDENTE'] != '-']\n",
    "list_idx_rebuild_location = list(df_empty_locations_with_address.index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rebuild 'sector catastral', 'UPZ', 'localidad', 'latitud', 'longitud' using web scraping\n",
    "url='https://mapas.bogota.gov.co'\n",
    "driver = ws_address.web_scrap_page(url)\n",
    "registers_to_process = len(list_idx_rebuild)\n",
    "rebuilt_registers = 0\n",
    "registers_not_processed = 0\n",
    "other_condition_counter = 0\n",
    "\n",
    "for index in list_idx_rebuild:\n",
    "    state = rebuild_location_in_nuse(merged_nuse, index, driver)\n",
    "    \n",
    "    if state == \"Rebuilt\":\n",
    "        rebuilt_registers += 1\n",
    "    elif state == \"Not processed\":\n",
    "        registers_not_processed += 1\n",
    "    else:\n",
    "        other_condition_counter += 1\n",
    "    \n",
    "    print('Rebuilt registers: ',rebuilt_registers,'/',registers_to_process)\n",
    "    print('Registers not processed: ',registers_not_processed, '/', registers_to_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rebuilt_registers)\n",
    "print(registers_not_processed)\n",
    "print(other_condition_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_nuse.to_csv(r'/Users/anamaria/Desktop/dev/security_project/datasets/rebuild_nuse_13112019.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
