{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "import calendar\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option('display.max_columns', None)  \n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import folium\n",
    "from folium import plugins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Merge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = '/Users/anamaria/Desktop/dev/security_project/datasets/NUSE 934 611(M) 2017-2018.dsv'\n",
    "data2018=pd.read_csv(data_location,delimiter=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = '/Users/anamaria/Desktop/dev/security_project/datasets/NUSE 934-611-611M ENERO2019.csv'\n",
    "data2019=pd.read_csv(data_location,delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [data2018, data2019]\n",
    "data = pd.concat(frames)\n",
    "merged_nuse = data.loc[data['TIPO_DETALLE'] == '934 - RIÑA']\n",
    "merged_nuse.reset_index(inplace=True)\n",
    "merged_nuse.drop(columns=['index'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_nuse.to_csv(r'/Users/anamaria/Desktop/dev/security_project/datasets/merged_nuse.csv',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Rebuild missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "localidadCodDictionaryNuse = {1:'USAQUEN',\n",
    "                              2:'CHAPINERO',\n",
    "                              3:'SANTA FE',\n",
    "                              4:'SAN CRISTOBAL',\n",
    "                              5:'USME',\n",
    "                              6:'TUNJUELITO',\n",
    "                              7:'BOSA',\n",
    "                              8:'KENNEDY',\n",
    "                              9:'FONTIBON',\n",
    "                              10:'ENGATIVA',\n",
    "                              11:'SUBA',\n",
    "                              12:'BARRIOS UNIDOS',\n",
    "                              13:'TEUSAQUILLO',\n",
    "                              14:'LOS MARTIRES',\n",
    "                              15:'ANTONIO NARIÑO',\n",
    "                              16:'PUENTE ARANDA',\n",
    "                              17:'CANDELARIA',\n",
    "                              18:'RAFAEL URIBE URIBE',\n",
    "                              19:'CIUDAD BOLIVAR',\n",
    "                              20:'SUMAPAZ',\n",
    "                              99:'SIN LOCALIZACION'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods to rebuild"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import ws_address\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import re\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_between( s, first, last ):\n",
    "    try:\n",
    "        start = s.index( first ) + len( first )\n",
    "        end = s.index( last, start )\n",
    "        return s[start:end]\n",
    "    except ValueError:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = [\"Dirección ingresada: \",\"Dirección encontrada: \",\"Tipo dirección: \",\"Código postal: \",\"Sector catastral: \",\n",
    "        \"UPZ: \",\"Localidad: \",\"Latitud: \",\"Longitud: \",\"CHIP: \"]\n",
    "def parse_address_ws(ws_result):\n",
    "    location = {}\n",
    "    for idx in range(len(tags)-1):\n",
    "        location[tags[idx].replace(': ','')] = find_between(ws_result,tags[idx],tags[idx+1])\n",
    "    return location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_upz(original_df,index,UPZ_ws_field):\n",
    "    original_df.at[index,'COD_UPZ'] = find_between(UPZ_ws_field, '(', ')')\n",
    "    original_df.at[index,'UPZ'] = find_between(UPZ_ws_field, '', ' (')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cod_localidad(localidad_name):\n",
    "    return [key  for (key, value) in localidadCodDictionaryNuse.items() if value == localidad_name][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebuild_location_in_nuse(original_df, index, driver):\n",
    "    address = original_df.at[index,'STR_DIRECCION_INCIDENTE']\n",
    "    print(address)\n",
    "    result_ws = ws_address.web_scrap_address(driver,address)\n",
    "    ws_address.delete_address(driver,address)\n",
    "    print(result_ws)\n",
    "\n",
    "    if result_ws != \"Not found\":\n",
    "        parsed_result = parse_address_ws(result_ws)\n",
    "        print(parsed_result)\n",
    "        original_df.at[index,'LATITUD'] = float(parsed_result['Latitud'])\n",
    "        original_df.at[index,'LONGITUD'] = float(parsed_result['Longitud'])\n",
    "        parsed_localidad = parsed_result['Localidad']\n",
    "        if parsed_localidad == 'ANTONIO NARIÑO':\n",
    "            original_df.at[index,'LOCALIDAD'] = parsed_localidad\n",
    "        else:\n",
    "            original_df.at[index,'LOCALIDAD'] = unidecode.unidecode(parsed_localidad)\n",
    "        original_df.at[index,'COD_LOCALIDAD'] = int(get_cod_localidad(original_df.at[index,'LOCALIDAD']))\n",
    "        original_df.at[index,'SEC_CATASTRAL'] = parsed_result['Sector catastral']\n",
    "        assign_upz(original_df,index,parsed_result['UPZ'])\n",
    "        return \"Rebuilt\"\n",
    "    else:\n",
    "        return \"Not processed\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebuild_address_in_nuse(original_df, index):\n",
    "    log_text = original_df.at[index,'LOG_TEXT']\n",
    "    address_found = re.search(address_regex,log_text)\n",
    "\n",
    "    if address_found != None:\n",
    "        parsed_address = clean_address(address_found)\n",
    "        print(parsed_address.strip())\n",
    "        original_df.at[index,'STR_DIRECCION_INCIDENTE'] = parsed_address.strip()\n",
    "        return \"Rebuilt\"\n",
    "    else:\n",
    "        original_df.at[index,'STR_DIRECCION_INCIDENTE'] = 'ND'\n",
    "        return \"Not processed\"\n",
    "\n",
    "def clean_address(address_found):\n",
    "    exclude_char_list = ['~','/','*','(',')']\n",
    "    one_occurrence = address_found.group().split(',,,')[0].replace(',',' ')\n",
    "    final_address = one_occurrence\n",
    "    \n",
    "    for char in exclude_char_list:\n",
    "        if char in one_occurrence:\n",
    "            final_address = final_address.split(char)[0]\n",
    "            \n",
    "    numbers_in_substring = re.sub('[^0-9]','', final_address)\n",
    "    numbers_proportion = len(numbers_in_substring)/len(final_address)\n",
    "    \n",
    "    if numbers_proportion < 0.2:\n",
    "        final_address = 'ND'\n",
    "    \n",
    "    return final_address\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement rebuild methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = '/Users/anamaria/Desktop/dev/security_project/datasets/merged_nuse.csv'\n",
    "merged_nuse=pd.read_csv(data_location,delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"Tipo de dato\":merged_nuse.dtypes.values,\n",
    "              \"Celdas con valor '-'\":(merged_nuse == '-').sum().values,\n",
    "              \"Celdas con valor ''\":(merged_nuse == '').sum().values,\n",
    "              \"Celdas con valor ' '\":(merged_nuse == ' ').sum().values,\n",
    "              \"Celdas vacías\": merged_nuse.isna().sum().values},\n",
    "             index=merged_nuse.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rebuild address through log_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try to rebuild missing address through log_text field\n",
    "df_empty_locations_without_address = merged_nuse.loc[merged_nuse['STR_DIRECCION_INCIDENTE'] == '-']\n",
    "list_idx_rebuild_address = list(df_empty_locations_without_address.index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_idx_rebuild_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address_regex= '(CL|DG|KR|TV)+\\s\\d+.*(,,)'\n",
    "registers_to_process = len(list_idx_rebuild_address)\n",
    "rebuilt_registers = 0\n",
    "registers_not_processed = 0\n",
    "other_condition_counter = 0\n",
    "\n",
    "for index in list_idx_rebuild_address:\n",
    "    state = rebuild_address_in_nuse(merged_nuse, index)\n",
    "    \n",
    "    if state == \"Rebuilt\":\n",
    "        rebuilt_registers += 1\n",
    "    elif state == \"Not processed\":\n",
    "        registers_not_processed += 1\n",
    "    else:\n",
    "        other_condition_counter += 1\n",
    "    \n",
    "    print('Rebuilt registers: ',rebuilt_registers,'/',registers_to_process)\n",
    "    print('Registers not processed: ',registers_not_processed, '/', registers_to_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_nuse.to_csv(r'/Users/anamaria/Desktop/dev/security_project/datasets/rebuild_address_nuse_18112019.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"Tipo de dato\":merged_nuse.dtypes.values,\n",
    "              \"Celdas con valor '-'\":(merged_nuse == '-').sum().values,\n",
    "              \"Celdas con valor 'ND'\":(merged_nuse == 'ND').sum().values,\n",
    "              \"Celdas vacías\": merged_nuse.isna().sum().values},\n",
    "             index=merged_nuse.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rebuild location through address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = '/Users/anamaria/Desktop/dev/security_project/datasets/rebuild_address_nuse_18112019.csv'\n",
    "df_input = pd.read_csv(data_location,delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_input.loc[df_input['COD_UPZ'] == '-']\n",
    "df2 = df_input.loc[df_input['UPZ'] == '-']\n",
    "df3 = df_input.loc[df_input['COD_SEC_CATAST'] == '-']\n",
    "df4 = df_input.loc[df_input['SEC_CATASTRAL'] == '-']\n",
    "df5 = df_input.loc[df_input['COD_BARRIO'] == '-']\n",
    "df6 = df_input.loc[df_input['BARRIO'] == '-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.equals(df2) and df1.equals(df3) and df1.equals(df4) and df1.equals(df5) and df1.equals(df6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try to rebuild 'sector catastral', 'UPZ', 'localidad', 'latitud', 'longitud' through address\n",
    "df_empty_locations_with_address = df1.loc[df1['STR_DIRECCION_INCIDENTE'] != 'ND']\n",
    "list_idx_rebuild_location = list(df_empty_locations_with_address.index.values)\n",
    "len(list_idx_rebuild_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rebuild 'sector catastral', 'UPZ', 'localidad', 'latitud', 'longitud' using web scraping\n",
    "df_output = df_input\n",
    "url='https://mapas.bogota.gov.co'\n",
    "driver = ws_address.web_scrap_page(url)\n",
    "registers_to_process = len(list_idx_rebuild_location)\n",
    "rebuilt_registers = 0\n",
    "registers_not_processed = 0\n",
    "other_condition_counter = 0\n",
    "\n",
    "for index in list_idx_rebuild_location:\n",
    "    state = rebuild_location_in_nuse(df_output, index, driver)\n",
    "    \n",
    "    if state == \"Rebuilt\":\n",
    "        rebuilt_registers += 1\n",
    "    elif state == \"Not processed\":\n",
    "        registers_not_processed += 1\n",
    "    else:\n",
    "        other_condition_counter += 1\n",
    "    \n",
    "    print('Rebuilt registers: ',rebuilt_registers,'/',registers_to_process)\n",
    "    print('Registers not processed: ',registers_not_processed, '/', registers_to_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rebuilt_registers)\n",
    "print(registers_not_processed)\n",
    "print(other_condition_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output.to_csv(r'/Users/anamaria/Desktop/dev/security_project/datasets/rebuild_locations_nuse_19112019.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"Tipo de dato\":merged_nuse.dtypes.values,\n",
    "              \"Celdas con valor '-'\":(merged_nuse == '-').sum().values,\n",
    "              \"Celdas con valor 'ND'\":(merged_nuse == 'ND').sum().values,\n",
    "              \"Celdas vacías\": merged_nuse.isna().sum().values},\n",
    "             index=merged_nuse.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Registers without address can not be rebuilt\n",
    "df_empty_locations_without_address = df1.loc[df1['STR_DIRECCION_INCIDENTE'] == 'ND']\n",
    "list_idx_not_rebuild = list(df_empty_locations_without_address.index.values)\n",
    "len(list_idx_not_rebuild)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: assign ND to df_empty_locations_without_address on location fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Standardise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = '/Users/anamaria/Desktop/dev/security_project/datasets/rebuild_address_nuse_18112019.csv'\n",
    "df_input = pd.read_csv(data_location,delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create timpestamp col to handle time ranges on unique event process\n",
    "df_input['time_stamp']=pd.to_datetime(df_input['FECHA'] + ' ' + df_input[\"HORA\"].astype(str).str.rjust(4,'0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "manzanas = gpd.read_file('/Users/anamaria/Desktop/dev/security_project/assets/manzanas.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cod manzana = 00252994\n",
    "#coord = 4.513061331326362, -74.1158798086992\n",
    "\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry.polygon import Polygon\n",
    "\n",
    "manzanas = manzanas[['CODIGO_MAN','geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lon = -74.1175\n",
    "lat = 4.5128\n",
    "point=Point(lon,lat)\n",
    "\n",
    "found = 'No encontrado'\n",
    "\n",
    "for man, geo in manzanas.values:\n",
    "    if point.within(geo):\n",
    "        found = man\n",
    "print(found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
