{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "import calendar\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option('display.max_columns', None)  \n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import folium\n",
    "from folium import plugins\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry.polygon import Polygon\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Merge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = '/Users/anamaria/Desktop/dev/security_project/datasets/NUSE 934 611(M) 2017-2018.dsv'\n",
    "data2018=pd.read_csv(data_location,delimiter=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = '/Users/anamaria/Desktop/dev/security_project/datasets/NUSE 934-611-611M ENERO2019.csv'\n",
    "data2019=pd.read_csv(data_location,delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [data2018, data2019]\n",
    "data = pd.concat(frames)\n",
    "merged_nuse = data.loc[data['TIPO_DETALLE'] == '934 - RIÑA']\n",
    "merged_nuse.reset_index(inplace=True)\n",
    "merged_nuse.drop(columns=['index'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_nuse.to_csv(r'/Users/anamaria/Desktop/dev/security_project/datasets/merged_nuse.csv',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Rebuild missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "localidadCodDictionaryNuse = {1:'USAQUEN',\n",
    "                              2:'CHAPINERO',\n",
    "                              3:'SANTA FE',\n",
    "                              4:'SAN CRISTOBAL',\n",
    "                              5:'USME',\n",
    "                              6:'TUNJUELITO',\n",
    "                              7:'BOSA',\n",
    "                              8:'KENNEDY',\n",
    "                              9:'FONTIBON',\n",
    "                              10:'ENGATIVA',\n",
    "                              11:'SUBA',\n",
    "                              12:'BARRIOS UNIDOS',\n",
    "                              13:'TEUSAQUILLO',\n",
    "                              14:'LOS MARTIRES',\n",
    "                              15:'ANTONIO NARIÑO',\n",
    "                              16:'PUENTE ARANDA',\n",
    "                              17:'CANDELARIA',\n",
    "                              18:'RAFAEL URIBE URIBE',\n",
    "                              19:'CIUDAD BOLIVAR',\n",
    "                              20:'SUMAPAZ',\n",
    "                              99:'SIN LOCALIZACION'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods to rebuild"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import ws_address\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import re\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_between( s, first, last ):\n",
    "    try:\n",
    "        start = s.index( first ) + len( first )\n",
    "        end = s.index( last, start )\n",
    "        return s[start:end]\n",
    "    except ValueError:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = [\"Dirección ingresada: \",\"Dirección encontrada: \",\"Tipo dirección: \",\"Código postal: \",\"Sector catastral: \",\n",
    "        \"UPZ: \",\"Localidad: \",\"Latitud: \",\"Longitud: \",\"CHIP: \"]\n",
    "def parse_address_ws(ws_result):\n",
    "    location = {}\n",
    "    for idx in range(len(tags)-1):\n",
    "        location[tags[idx].replace(': ','')] = find_between(ws_result,tags[idx],tags[idx+1])\n",
    "    return location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_upz(original_df,index,UPZ_ws_field):\n",
    "    original_df.at[index,'COD_UPZ'] = find_between(UPZ_ws_field, '(', ')')\n",
    "    original_df.at[index,'UPZ'] = find_between(UPZ_ws_field, '', ' (')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cod_localidad(localidad_name):\n",
    "    return [key  for (key, value) in localidadCodDictionaryNuse.items() if value == localidad_name][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebuild_location_in_nuse(original_df, index, driver):\n",
    "    address = original_df.at[index,'STR_DIRECCION_INCIDENTE']\n",
    "    print(address)\n",
    "    result_ws = ws_address.web_scrap_address(driver,address)\n",
    "    ws_address.delete_address(driver,address)\n",
    "    print(result_ws)\n",
    "\n",
    "    if result_ws != \"Not found\":\n",
    "        parsed_result = parse_address_ws(result_ws)\n",
    "        print(parsed_result)\n",
    "        original_df.at[index,'LATITUD'] = float(parsed_result['Latitud'])\n",
    "        original_df.at[index,'LONGITUD'] = float(parsed_result['Longitud'])\n",
    "        parsed_localidad = parsed_result['Localidad']\n",
    "        if parsed_localidad == 'ANTONIO NARIÑO':\n",
    "            original_df.at[index,'LOCALIDAD'] = parsed_localidad\n",
    "        else:\n",
    "            original_df.at[index,'LOCALIDAD'] = unidecode.unidecode(parsed_localidad)\n",
    "        original_df.at[index,'COD_LOCALIDAD'] = int(get_cod_localidad(original_df.at[index,'LOCALIDAD']))\n",
    "        original_df.at[index,'SEC_CATASTRAL'] = parsed_result['Sector catastral']\n",
    "        assign_upz(original_df,index,parsed_result['UPZ'])\n",
    "        return \"Rebuilt\"\n",
    "    else:\n",
    "        return \"Not processed\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebuild_address_in_nuse(original_df, index):\n",
    "    log_text = original_df.at[index,'LOG_TEXT']\n",
    "    address_found = re.search(address_regex,log_text)\n",
    "\n",
    "    if address_found != None:\n",
    "        parsed_address = clean_address(address_found)\n",
    "        print(parsed_address.strip())\n",
    "        original_df.at[index,'STR_DIRECCION_INCIDENTE'] = parsed_address.strip()\n",
    "        return \"Rebuilt\"\n",
    "    else:\n",
    "        original_df.at[index,'STR_DIRECCION_INCIDENTE'] = 'ND'\n",
    "        return \"Not processed\"\n",
    "\n",
    "def clean_address(address_found):\n",
    "    exclude_char_list = ['~','/','*','(',')']\n",
    "    one_occurrence = address_found.group().split(',,,')[0].replace(',',' ')\n",
    "    final_address = one_occurrence\n",
    "    \n",
    "    for char in exclude_char_list:\n",
    "        if char in one_occurrence:\n",
    "            final_address = final_address.split(char)[0]\n",
    "            \n",
    "    numbers_in_substring = re.sub('[^0-9]','', final_address)\n",
    "    numbers_proportion = len(numbers_in_substring)/len(final_address)\n",
    "    \n",
    "    if numbers_proportion < 0.2:\n",
    "        final_address = 'ND'\n",
    "    \n",
    "    return final_address\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement rebuild methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = '/Users/anamaria/Desktop/dev/security_project/datasets/merged_nuse.csv'\n",
    "merged_nuse=pd.read_csv(data_location,delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"Tipo de dato\":merged_nuse.dtypes.values,\n",
    "              \"Celdas con valor '-'\":(merged_nuse == '-').sum().values,\n",
    "              \"Celdas con valor ''\":(merged_nuse == '').sum().values,\n",
    "              \"Celdas con valor ' '\":(merged_nuse == ' ').sum().values,\n",
    "              \"Celdas vacías\": merged_nuse.isna().sum().values},\n",
    "             index=merged_nuse.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rebuild address through log_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try to rebuild missing address through log_text field\n",
    "df_empty_locations_without_address = merged_nuse.loc[merged_nuse['STR_DIRECCION_INCIDENTE'] == '-']\n",
    "list_idx_rebuild_address = list(df_empty_locations_without_address.index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_idx_rebuild_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address_regex= '(CL|DG|KR|TV)+\\s\\d+.*(,,)'\n",
    "registers_to_process = len(list_idx_rebuild_address)\n",
    "rebuilt_registers = 0\n",
    "registers_not_processed = 0\n",
    "other_condition_counter = 0\n",
    "\n",
    "for index in list_idx_rebuild_address:\n",
    "    state = rebuild_address_in_nuse(merged_nuse, index)\n",
    "    \n",
    "    if state == \"Rebuilt\":\n",
    "        rebuilt_registers += 1\n",
    "    elif state == \"Not processed\":\n",
    "        registers_not_processed += 1\n",
    "    else:\n",
    "        other_condition_counter += 1\n",
    "    \n",
    "    print('Rebuilt registers: ',rebuilt_registers,'/',registers_to_process)\n",
    "    print('Registers not processed: ',registers_not_processed, '/', registers_to_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_nuse.to_csv(r'/Users/anamaria/Desktop/dev/security_project/datasets/rebuild_address_nuse_18112019.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"Tipo de dato\":merged_nuse.dtypes.values,\n",
    "              \"Celdas con valor '-'\":(merged_nuse == '-').sum().values,\n",
    "              \"Celdas con valor 'ND'\":(merged_nuse == 'ND').sum().values,\n",
    "              \"Celdas vacías\": merged_nuse.isna().sum().values},\n",
    "             index=merged_nuse.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rebuild location through address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = '/Users/anamaria/Desktop/dev/security_project/datasets/rebuild_address_nuse_18112019.csv'\n",
    "df_input = pd.read_csv(data_location,delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_input.loc[df_input['COD_UPZ'] == '-']\n",
    "df2 = df_input.loc[df_input['UPZ'] == '-']\n",
    "df3 = df_input.loc[df_input['COD_SEC_CATAST'] == '-']\n",
    "df4 = df_input.loc[df_input['SEC_CATASTRAL'] == '-']\n",
    "df5 = df_input.loc[df_input['COD_BARRIO'] == '-']\n",
    "df6 = df_input.loc[df_input['BARRIO'] == '-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.equals(df2) and df1.equals(df3) and df1.equals(df4) and df1.equals(df5) and df1.equals(df6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try to rebuild 'sector catastral', 'UPZ', 'localidad', 'latitud', 'longitud' through address\n",
    "df_empty_locations_with_address = df1.loc[df1['STR_DIRECCION_INCIDENTE'] != 'ND']\n",
    "list_idx_rebuild_location = list(df_empty_locations_with_address.index.values)\n",
    "len(list_idx_rebuild_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rebuild 'sector catastral', 'UPZ', 'localidad', 'latitud', 'longitud' using web scraping\n",
    "df_output = df_input\n",
    "url='https://mapas.bogota.gov.co'\n",
    "driver = ws_address.web_scrap_page(url)\n",
    "registers_to_process = len(list_idx_rebuild_location)\n",
    "rebuilt_registers = 0\n",
    "registers_not_processed = 0\n",
    "other_condition_counter = 0\n",
    "\n",
    "for index in list_idx_rebuild_location:\n",
    "    state = rebuild_location_in_nuse(df_output, index, driver)\n",
    "    \n",
    "    if state == \"Rebuilt\":\n",
    "        rebuilt_registers += 1\n",
    "    elif state == \"Not processed\":\n",
    "        registers_not_processed += 1\n",
    "    else:\n",
    "        other_condition_counter += 1\n",
    "    \n",
    "    print('Rebuilt registers: ',rebuilt_registers,'/',registers_to_process)\n",
    "    print('Registers not processed: ',registers_not_processed, '/', registers_to_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rebuilt_registers)\n",
    "print(registers_not_processed)\n",
    "print(other_condition_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output.to_csv(r'/Users/anamaria/Desktop/dev/security_project/datasets/rebuild_locations_nuse_19112019.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"Tipo de dato\":merged_nuse.dtypes.values,\n",
    "              \"Celdas con valor '-'\":(merged_nuse == '-').sum().values,\n",
    "              \"Celdas con valor 'ND'\":(merged_nuse == 'ND').sum().values,\n",
    "              \"Celdas vacías\": merged_nuse.isna().sum().values},\n",
    "             index=merged_nuse.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Registers without address can not be rebuilt\n",
    "df_empty_locations_without_address = df1.loc[df1['STR_DIRECCION_INCIDENTE'] == 'ND']\n",
    "list_idx_not_rebuild = list(df_empty_locations_without_address.index.values)\n",
    "len(list_idx_not_rebuild)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: assign ND to df_empty_locations_without_address on location fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify registers are inside Bogotá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loc_from_coor(lat,lon):\n",
    "    \n",
    "    point=Point(lon,lat)\n",
    "    for loc, geo in loc_.values:\n",
    "        if point.within(geo[0]):\n",
    "            return loc\n",
    "    return \"No Encontrado\"\n",
    "\n",
    "def get_upz_from_coor(lat,lon):\n",
    "\n",
    "    point=Point(lon,lat)\n",
    "\n",
    "    for cod,upz, geo in upz_.values:\n",
    "        if point.within(geo):\n",
    "            return cod,upz\n",
    "    return \"No Encontrado\"\n",
    "\n",
    "def get_loc_first(permutaciones):\n",
    "\n",
    "    for p in permutaciones:\n",
    "        localidad=get_loc_from_coor(p[0],p[1])\n",
    "        if localidad != 'No Encontrado':\n",
    "            break\n",
    "    return localidad,p[0],p[1]\n",
    "\n",
    "def get_loc_from_shape(lat,lon):\n",
    "    \n",
    "    ly=lat\n",
    "    lx=lon\n",
    "    permutaciones=[(ly , lx),(-ly , lx),(-ly, -lx),(ly , -lx),(lx , ly),(-lx , ly),(-lx, -ly),(lx , -ly)]\n",
    "    return list(get_loc_first(permutaciones))\n",
    "\n",
    "def recover_coors(df,col_lat_y,col_lon_x):\n",
    "    coors=df[[col_lat_y,col_lon_x]].values\n",
    "    new_data=[]\n",
    "    for i in coors:\n",
    "        row=get_loc_from_shape(i[0],i[1])\n",
    "        __UPZ__=get_upz_from_coor(row[1],row[2])\n",
    "        if len(__UPZ__) > 2:\n",
    "            row+=[__UPZ__]*2\n",
    "        else:\n",
    "            row+=__UPZ__\n",
    "        new_data.append(row)\n",
    "    new_data=pd.DataFrame(new_data,columns=[\"LOCALIDAD_from_coors\",'new_'+col_lat_y,'new_'+col_lon_x,'cod_UPZ','upz_from_coors'])\n",
    "\n",
    "    return df.join(new_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = '/Users/anamaria/Desktop/dev/security_project/datasets/rebuild_address_nuse_18112019.csv'\n",
    "data = pd.read_csv(data_location,delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_geo=\"/Users/anamaria/Desktop/dev/security_project/assets/loca.geojson.json\"\n",
    "loc_=gpd.read_file(loc_geo)\n",
    "loc_=loc_[[\"LocNombre\",'geometry']]\n",
    "\n",
    "upz_geo=\"/Users/anamaria/Desktop/dev/security_project/assets/upla.json\"\n",
    "upz_=gpd.read_file(upz_geo)\n",
    "upz_=upz_[[\"UPlCodigo\",'UPlNombre','geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_lat='LATITUD'\n",
    "col_lon='LONGITUD'\n",
    "col_localidad='LOCALIDAD'\n",
    "col_direccion='STR_DIRECCION_INCIDENTE'\n",
    "cod_loc='COD_LOCALIDAD'\n",
    "col_upz='UPZ'\n",
    "cod_upz='COD_UPZ'\n",
    "col_barrio='BARRIO'\n",
    "\n",
    "Ubicacion=data[[col_lat,col_lon,col_localidad]]\n",
    "new_Ubicacion=recover_coors(Ubicacion,col_lat,col_lon)\n",
    "\n",
    "index_no_loc_out_coor=data[(data[col_direccion] == '-') & (data[cod_loc] == 99) & (new_Ubicacion.LOCALIDAD_from_coors == 'No Encontrado')].index\n",
    "index_perm=data[(new_Ubicacion.LOCALIDAD_from_coors != 'No Encontrado') ].index\n",
    "\n",
    "LOC_COD=data[[col_localidad,cod_loc]].drop_duplicates()\n",
    "LOC_COD_={}\n",
    "\n",
    "for i in LOC_COD.values:\n",
    "    LOC_COD_[i[0]]=i[1]\n",
    "\n",
    "for i in index_perm:\n",
    "    data.at[i,col_localidad] = new_Ubicacion.loc[i]['LOCALIDAD_from_coors']\n",
    "    data.at[i,cod_loc] = LOC_COD_[new_Ubicacion.loc[i]['LOCALIDAD_from_coors']]\n",
    "    data.at[i,col_lon] = new_Ubicacion.loc[i]['new_'+col_lon]\n",
    "    data.at[i,col_lat] = new_Ubicacion.loc[i]['new_'+col_lat]\n",
    "    data.at[i,col_upz] = new_Ubicacion.loc[i]['upz_from_coors']\n",
    "    data.at[i,cod_upz] = new_Ubicacion.loc[i]['cod_UPZ']\n",
    "\n",
    "index_input_from_dir=data[(data[cod_loc] != 99) &  (data[col_direccion] != '-') & (new_Ubicacion.LOCALIDAD_from_coors == 'No Encontrado') ].index\n",
    "\n",
    "  \n",
    "index_no_dir_coor_out=data[(data[cod_loc] != 99) &  (data[col_direccion] == '-') & (new_Ubicacion.LOCALIDAD_from_coors != data[col_localidad])].index\n",
    "\n",
    "\n",
    "data=data.drop(index_no_loc_out_coor)\n",
    "data=data.drop(index_no_dir_coor_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_Ubicacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(r'/Users/anamaria/Desktop/dev/security_project/datasets/verify_coor_bog_nuse_25112019.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(r'/Users/anamaria/Desktop/dev/security_project/datasets/new_ubicacion_nuse_25112019.csv',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Standardise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = '/Users/anamaria/Desktop/dev/security_project/datasets/rebuild_address_nuse_18112019.csv'\n",
    "df_input = pd.read_csv(data_location,delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create timpestamp col to handle time ranges on unique event process\n",
    "df_input['time_stamp']=pd.to_datetime(df_input['FECHA'] + ' ' + df_input[\"HORA\"].astype(str).str.rjust(4,'0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"Tipo de dato\":df_input.dtypes.values,\n",
    "              \"Celdas con valor '-'\":(df_input == '-').sum().values,\n",
    "              \"Celdas con valor 'ND'\":(df_input == 'ND').sum().values,\n",
    "              \"Celdas vacías\": df_input.isna().sum().values},\n",
    "             index=df_input.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 One register per event: event that occurs within 400 mts radius and 20 minutes time interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find duplicated events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, datetime\n",
    "time_offset = 20\n",
    "coor_offset = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_duplicated_events(df, row):\n",
    "    current_time = row['time_stamp']\n",
    "    current_lat = row['LATITUD']\n",
    "    current_lon = row['LONGITUD']\n",
    "    current_point=Point(current_lon,current_lat)\n",
    "\n",
    "    duplicated_event_idx = {}\n",
    "    limit_time_interval = current_time + datetime.timedelta(minutes = time_offset)\n",
    "    df_event_time = df.loc[(df['time_stamp'] >= current_time) & (df['time_stamp'] < limit_time_interval)]\n",
    "    \n",
    "    lat_point_list = [current_lat-coor_offset, current_lat-coor_offset, current_lat+coor_offset, current_lat+coor_offset]\n",
    "    lon_point_list = [current_lon+coor_offset, current_lon-coor_offset, current_lon-coor_offset, current_lon+coor_offset]\n",
    "    polygon_event = Polygon(zip(lon_point_list, lat_point_list))\n",
    "    \n",
    "    for index, row in df_event_time.iterrows():\n",
    "        point=Point(row['LONGITUD'],row['LATITUD'])\n",
    "        if point.within(polygon_event):\n",
    "            #duplicated_event_idx.append(index)\n",
    "            duplicated_event_idx[index] = row['STR_NUMERO_INTERNO']\n",
    "    return duplicated_event_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output = df_input.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output['dup_event'] = df_output.apply (lambda row: find_duplicated_events(df_output, row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output.to_csv(r'/Users/anamaria/Desktop/dev/security_project/datasets/standardise_find_dup_event_nuse_26112019.csv',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete duplicated events: preserve the first event on dup_event column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = '/Users/anamaria/Desktop/dev/security_project/datasets/standardise_find_dup_event_nuse_26112019.csv'\n",
    "df_input = pd.read_csv(data_location,delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"Tipo de dato\":df_input.dtypes.values,\n",
    "              \"Celdas con valor '-'\":(df_input == '-').sum().values,\n",
    "              \"Celdas con valor 'ND'\":(df_input == 'ND').sum().values,\n",
    "              \"Celdas vacías\": df_input.isna().sum().values},\n",
    "             index=df_input.columns)\n",
    "print(df_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Get index of registers that should be deleted\n",
    "import ast\n",
    "df = df_input\n",
    "list_idx_repeated = []\n",
    "list_idx_preserved = []\n",
    "registers_to_process = len(df)\n",
    "list_idx_processed =[]\n",
    "counter_processed = 0\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    dup_event_x = ast.literal_eval(df.at[index,'dup_event'])\n",
    "    current_dup_events = list(dup_event_x.keys())\n",
    "\n",
    "    if (current_dup_events[0] not in list_idx_processed) & (current_dup_events[0] not in list_idx_preserved):\n",
    "        list_idx_preserved.append(current_dup_events[0])\n",
    "        list_idx_processed.append(current_dup_events[0])\n",
    "        current_dup_events.pop(0)\n",
    "\n",
    "    for idx_event in current_dup_events:\n",
    "        if idx_event not in list_idx_processed:\n",
    "            list_idx_repeated.append(idx_event)\n",
    "            list_idx_processed.append(idx_event)\n",
    "                \n",
    "    counter_processed += 1\n",
    "    \n",
    "    print('Registers processed: ',counter_processed,'/',registers_to_process)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check (quantitatively) ID of preserved and repeated events index was succesful\n",
    "print(len(list_idx_repeated)+len(list_idx_preserved))\n",
    "print(len(list_idx_processed))\n",
    "join_list = list_idx_preserved + list_idx_repeated\n",
    "\n",
    "import collections\n",
    "seen = set()\n",
    "uniq = []\n",
    "for x in join_list:\n",
    "    if x not in seen:\n",
    "        uniq.append(x)\n",
    "        seen.add(x)\n",
    "\n",
    "print(len(uniq))\n",
    "\n",
    "lst = join_list\n",
    "dupItems = []\n",
    "uniqItems = {}\n",
    "for x in lst:\n",
    "    if x not in uniqItems:\n",
    "        uniqItems[x] = 1\n",
    "    else:\n",
    "        if uniqItems[x] == 1:\n",
    "            dupItems.append(x)\n",
    "        uniqItems[x] += 1\n",
    "        \n",
    "print(len(dupItems))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output = df_input.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output=df_output.drop(list_idx_repeated)\n",
    "df_output.drop(columns=['dup_event','time_stamp'],inplace=True)\n",
    "df_output.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output.to_csv(r'/Users/anamaria/Desktop/dev/security_project/datasets/standardise_result_nuse_27112019.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save lists\n",
    "MyFile=open('/Users/anamaria/Desktop/dev/security_project/datasets/list_idx_preserved_27112019.txt','w')\n",
    "MyList=map(lambda x: str(x)+'\\n', list_idx_preserved)\n",
    "MyFile.writelines(MyList)\n",
    "MyFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save lists\n",
    "MyFile=open('/Users/anamaria/Desktop/dev/security_project/datasets/list_idx_repeated_27112019.txt','w')\n",
    "MyList=map(lambda x: str(x)+'\\n', list_idx_repeated)\n",
    "MyFile.writelines(MyList)\n",
    "MyFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
